{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatteoOnger/algo-collusion-mm/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMTOP3utEbcK"
      },
      "source": [
        "# Algorithmic Collusion in Market Making - EXP3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGZMs7D1J3fF"
      },
      "source": [
        "A notebook testing various EXP3 agents implementing market-making strategies in the Glosten-Milgrom environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxmHCO7rFONX"
      },
      "source": [
        "## Notebook Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DResD4xyGhBX"
      },
      "source": [
        "### Colab Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOP-nvvbEYvD"
      },
      "outputs": [],
      "source": [
        "# Do NOT run this cell in local environment - it's intended for Google Colab only.\n",
        "\n",
        "# Clone GitHub repository\n",
        "!git clone https://github.com/MatteoOnger/algo-collusion-mm.git\n",
        "\n",
        "# Install dependencies\n",
        "!pip install --quiet -r /content/algo-collusion-mm/requirements.txt\n",
        "\n",
        "# Set working directory\n",
        "%cd /content/algo-collusion-mm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udKsn5GQHHt_"
      },
      "source": [
        "### Local Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOKNp7tDFKHo"
      },
      "outputs": [],
      "source": [
        "# Do NOT run this cell in Google Colab - it's intended for local Jupyter Notebooks only.\n",
        "\n",
        "# Autoreload imports\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# Select interactive backend for matplotlib\n",
        "%matplotlib widget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1Ulv9GBHZaH"
      },
      "source": [
        "## Main Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxDHqYnXrDzE"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import src.utils.plots as plots\n",
        "import src.utils.storage as storage\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from src.agents.agent import Agent\n",
        "from src.agents.makers.exp3 import MakerEXP3\n",
        "from src.agents.traders.nopass import NoPassTrader\n",
        "from src.envs import GMEnv\n",
        "from src.utils.stats import OnlineVectorStats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3GakIjfrFfE"
      },
      "outputs": [],
      "source": [
        "def scale_rewards_array(\n",
        "    cumulative_rewards: np.ndarray|float,\n",
        "    n_agents: int = 2,\n",
        "    n_episodes: int = 1,\n",
        "    min_reward: float = -0.5,\n",
        "    max_reward: float =  0.5\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Scales a NumPy array of cumulative rewards from the range [r_min, r_max] to [-1, +1].\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    cumulative_rewards : np.ndarray or float\n",
        "        Array of cumulative rewards to be scaled.\n",
        "    n_agents : int, default=2\n",
        "        Number of agents.\n",
        "    n_episodes : int, default=1\n",
        "        Number of episodes used to compute the cumulative rewards.\n",
        "    min_reward : float, default=-0.5\n",
        "        Expected minimum reward per episode in the single-agent case.\n",
        "    max_reward : float, default=0.5\n",
        "        Expected maximum reward per episode in the single-agent case.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    : np.ndarray\n",
        "        Array of rewards scaled to the range [-1, +1].\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If min_reward and max_reward are equal (to avoid division by zero).\n",
        "    \"\"\"\n",
        "    if min_reward == max_reward:\n",
        "        raise ValueError('min_reward and max_reward must be different to avoid division by zero')\n",
        "\n",
        "    r_min = n_episodes * min_reward\n",
        "    r_max = n_episodes * max_reward / n_agents\n",
        "\n",
        "    scaled = -1 + (cumulative_rewards - r_min) * 2 / (r_max - r_min)\n",
        "    return scaled\n",
        "\n",
        "\n",
        "def split_array(arr: np.ndarray, window_size: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Split an array into sub-arrays of fixed window size along the last axis.\n",
        "\n",
        "    If `window_size` is non-positive, the array is reshaped so that the last\n",
        "    axis becomes a single window of length equal to its size.\n",
        "    This is useful, for example, to ensure a consistent 3D shape\n",
        "    when no actual splitting is performed.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    arr : np.ndarray\n",
        "        Input array to be split.\n",
        "    window_size : int\n",
        "        Size of each window. Must be a positive integer.\n",
        "        If <= 0, the array is reshaped to (..., 1, N), where N is the\n",
        "        original length of the last axis.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    : np.ndarray\n",
        "        Reshaped array with shape (..., n_windows, window_size).\n",
        "        If `window_size <= 0`, returns the original array.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If `window_size` is not a divisor of the length of the last axis.\n",
        "    \"\"\"\n",
        "    if window_size <= 0:\n",
        "        return arr.reshape(arr.shape[:-1] + (1, -1))\n",
        "    return arr.reshape(arr.shape[:-1] + (-1, window_size))\n",
        "\n",
        "\n",
        "def get_calvano_collusion_index(rewards: np.ndarray, nash_reward: float, coll_reward: float, window_size: int = 0) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute the Calvano Collusion Index (CCI) from agent rewards.\n",
        "\n",
        "    The CCI measures the degree of collusion relative to Nash equilibrium\n",
        "    and perfect collusion benchmarks. Rewards are optionally aggregated\n",
        "    over fixed-size windows before computing the index.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    rewards : np.ndarray\n",
        "        Array of shape (n_agents, n_episodes) containing per-agent rewards.\n",
        "    nash_reward : float\n",
        "        Benchmark reward under Nash equilibrium (total across all agents).\n",
        "    coll_reward : float\n",
        "        Benchmark reward under perfect collusion (total across all agents).\n",
        "    window_size : int, default=0\n",
        "        Size of the episode window for reward aggregation.\n",
        "        If 0, no windowing is applied.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    : np.ndarray\n",
        "        Array of CCI values per agent and per window.\n",
        "\n",
        "    See Also\n",
        "    --------\n",
        "    - Calvano, E., Calzolari, G., Denicolò, V., & Pastorello, S. (2020).\n",
        "    Artificial intelligence, algorithmic pricing, and collusion.\n",
        "    *American Economic Review, 110*(10), 3267–3297.\n",
        "    https://doi.org/10.1257/aer.20190623\n",
        "    \"\"\"\n",
        "    nash_reward /= len(rewards)\n",
        "    coll_reward /= len(rewards)\n",
        "\n",
        "    rewards = split_array(rewards, window_size)\n",
        "    avg_rewards = rewards.mean(axis=-1)\n",
        "\n",
        "    cci = (avg_rewards - nash_reward) / (coll_reward - nash_reward)\n",
        "    return cci"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "saver = storage.ExperimentStorage('./experiments')\n",
        "\n",
        "objects = saver.load_objects('')\n",
        "print(f'Objects loaded: {list(objects.keys())}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multiple Runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epsilons = np.arange(1, 100) / 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80Oq4lNWrKcl"
      },
      "outputs": [],
      "source": [
        "r = 99              # Number of experiments\n",
        "n = 50_000          # Number of episodes\n",
        "k = 100             # Number of windows\n",
        "w = n // k          # Window size\n",
        "\n",
        "nash_reward = 0.1   # Nash reward\n",
        "coll_reward = 0.5   # Collusive reward\n",
        "\n",
        "# Prices and action space of the market makers\n",
        "action_space = np.array([[0., 0.], [.6, .4], [1., 0.]])\n",
        "\n",
        "# Min CCI of the last window per each experiment\n",
        "final_cci = np.zeros(r)\n",
        "# Min cumulative rewards of the last window per each experiment\n",
        "final_cum_rewards = np.zeros(r)\n",
        "\n",
        "# To save experimental results\n",
        "saver = storage.ExperimentStorage('./experiments/exp3/varying_epsilon_2/50k')\n",
        "# To compute online statistics\n",
        "stats_cci = OnlineVectorStats(2)\n",
        "\n",
        "start_time = time.time()\n",
        "current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "saver.print_and_save(f'Started at {current_time}')\n",
        "\n",
        "for i in range(r):\n",
        "    if i % 10 == 0:\n",
        "        saver.print_and_save(f'Running {i} ...')\n",
        "\n",
        "    agents: dict[str, Agent] = {\n",
        "        'maker_u_0': MakerEXP3(epsilon=epsilons[i], action_space=action_space, name='maker_u_0'),\n",
        "        'maker_u_1': MakerEXP3(epsilon=epsilons[i], action_space=action_space, name='maker_u_1'),\n",
        "        'trader_0': NoPassTrader(name='trader_0'),\n",
        "    }\n",
        "\n",
        "    env = GMEnv(\n",
        "        generate_vt = lambda: 0.5,\n",
        "        n_episodes = n,\n",
        "        n_makers_u = 2,\n",
        "        n_makers_i = 0,\n",
        "        n_traders = 1,\n",
        "    )\n",
        "\n",
        "    _, info = env.reset()\n",
        "\n",
        "    for agent in env.agent_iter():\n",
        "        action = agents[agent].act(env.observe(agent))\n",
        "        _, rewards, _, _, infos = env.step(action)\n",
        "\n",
        "        if infos['episode_finished']:\n",
        "            for a in env.possible_agents:\n",
        "                agents[a].update(rewards[a], infos[a])\n",
        "\n",
        "    cci = get_calvano_collusion_index(\n",
        "        np.array([agent.history.get_rewards() for name, agent in agents.items() if name in env.makers]),\n",
        "        nash_reward = nash_reward,\n",
        "        coll_reward = coll_reward,\n",
        "        window_size = w\n",
        "    )\n",
        "\n",
        "    info = {\n",
        "        'parmas' : {\n",
        "            'n_episodes' : n,\n",
        "            'window_size' : w,\n",
        "            'action_space' : str(action_space).replace('\\n', ','),\n",
        "            'epsilon' : epsilons[i],\n",
        "            'agent_type' : [agent.__class__.__name__ for agent in agents.values()]\n",
        "        },\n",
        "        'most_common_action' : {\n",
        "            n//w : {name : str(agent.history.compute_most_common(slice(-w, None))) for name, agent in agents.items() if name in env.makers}\n",
        "        },\n",
        "        'cumulative_rewards' : {\n",
        "            0  : {name : round(float(agent.history.get_rewards(slice(0, w)).sum()), 3) for name, agent in agents.items()},\n",
        "            n//w : {name : round(float(agent.history.get_rewards(slice(-w, None)).sum()), 3) for name, agent in agents.items()},\n",
        "            'global' : env.cumulative_rewards\n",
        "        },\n",
        "        'cci' : {\n",
        "            0  : {name : round(float(cci[idx, 0]), 3) for idx, name in enumerate(env.makers)},\n",
        "            n//w  : {name : round(float(cci[idx, -1]), 3) for idx, name in enumerate(env.makers)},\n",
        "            'global' : {name : round(float(cci[idx, :].mean()), 3) for idx, name in enumerate(env.makers)},\n",
        "        },\n",
        "        'seed' : {\n",
        "            name : agent._seed for name, agent in agents.items()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    stats_cci.update(cci[:, -1])\n",
        "    final_cci[i] = cci[:, -1].min()\n",
        "    final_cum_rewards[i] = np.array([agent.history.get_rewards(slice(-w, None)).sum() for name, agent in agents.items() if name in env.makers]).min()\n",
        "    \n",
        "    dir = saver.save_experiment([env] + list(agents.values()), info=info)\n",
        "    saver.print_and_save(f'{i:03} {\"*\" if cci[0, -1] >= 0.9 or cci[1, -1] >= 0.9 else \" \"} -> CCI:{info[\"cci\"][n//w]}'.ljust(60) + f' ({dir})')\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "saver.print_and_save(f'Done at {current_time} | Execution time: {execution_time:.2f} seconds')\n",
        "\n",
        "saver.save_objects({'final_cci': final_cci, 'final_cum_rewards': final_cum_rewards})\n",
        "saver.print_and_save(\n",
        "    f'Results:\\n'\n",
        "    f'- [CCI] Average in the last window: {np.round(stats_cci.get_mean(), 4)}\\n'\n",
        "    f'- [CCI] Standard deviation in the last window: {np.round(stats_cci.get_std(), 4)}'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Single Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "saver = storage.ExperimentStorage('./experiments')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n = 10_000       # Number of episodes\n",
        "w = 50           # Window size\n",
        "k = n // w       # Number of windows\n",
        "\n",
        "nash_reward = 0.1  # Nash reward\n",
        "coll_reward = 0.5  # Collusive reward\n",
        "\n",
        "counter = 0        # Number of episodes done\n",
        "\n",
        "# Prices and action space of the market makers\n",
        "action_space = np.array([[0.0, 0.0], [.6, .4], [1., 0.]])\n",
        "\n",
        "agents: dict[str, Agent] = {\n",
        "    'maker_u_0': MakerEXP3(epsilon=MakerEXP3.compute_epsilon(len(action_space), n), action_space=action_space, name='maker_u_0'),\n",
        "    'maker_u_1': MakerEXP3(epsilon=MakerEXP3.compute_epsilon(len(action_space), n), action_space=action_space, name='maker_u_1'),\n",
        "    'trader_0': NoPassTrader(name='trader_0'),\n",
        "}\n",
        "\n",
        "env = GMEnv(\n",
        "    generate_vt = lambda: 0.5,\n",
        "    n_episodes = n,\n",
        "    n_makers_u = 2,\n",
        "    n_makers_i = 0,\n",
        "    n_traders = 1,\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "print(f'Started at {current_time}')\n",
        "\n",
        "_, info = env.reset()\n",
        "for agent in env.agent_iter():\n",
        "    action = agents[agent].act(env.observe(agent))\n",
        "    _, rewards, _, _, infos = env.step(action)\n",
        "\n",
        "    if infos['episode_finished']:\n",
        "        if counter % 10_000 == 0:\n",
        "            print(f'Running episode {counter} ...')\n",
        "\n",
        "        for a in env.possible_agents:\n",
        "            if a in env.makers and counter % (k//5) == 0:\n",
        "                agents[a].history.record_extra(agents[a].weights.copy())\n",
        "            \n",
        "            agents[a].update(rewards[a], infos[a])\n",
        "    \n",
        "        counter += 1\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "print(f'Done at {current_time} | Execution time: {execution_time:.2f} seconds')\n",
        "\n",
        "cci = get_calvano_collusion_index(\n",
        "    np.array([agent.history.get_rewards() for name, agent in agents.items() if name in env.makers]),\n",
        "    nash_reward = nash_reward,\n",
        "    coll_reward = coll_reward,\n",
        "    window_size = w\n",
        ")\n",
        "\n",
        "info = {\n",
        "    'parmas' : {\n",
        "        'n_episodes' : n,\n",
        "        'window_size' : w,\n",
        "        'action_space' : str(action_space).replace('\\n', ','),\n",
        "        'agent_type' : [agent.__class__.__name__ for agent in agents.values()],\n",
        "    },\n",
        "    'most_common_action' : {\n",
        "        n//w : {name : str(agent.history.compute_most_common(slice(-w, None))) for name, agent in agents.items() if name in env.makers}\n",
        "    },\n",
        "    'cumulative_rewards' : {\n",
        "        0  : {name : round(float(agent.history.get_rewards(slice(0, w)).sum()), 3) for name, agent in agents.items()},\n",
        "        n//w : {name : round(float(agent.history.get_rewards(slice(-w, None)).sum()), 3) for name, agent in agents.items()},\n",
        "        'global' : env.cumulative_rewards\n",
        "    },\n",
        "    'cci' : {\n",
        "        0  : {name : round(float(cci[idx, 0]), 3) for idx, name in enumerate(env.makers)},\n",
        "        n//w  : {name : round(float(cci[idx, -1]), 3) for idx, name in enumerate(env.makers)},\n",
        "        'global' : {name : round(float(cci[idx, :].mean()), 3) for idx, name in enumerate(env.makers)},\n",
        "    },\n",
        "    'seed' : {\n",
        "        name : agent._seed for name, agent in agents.items()\n",
        "    }\n",
        "}\n",
        "\n",
        "fig = plots.plot_all(\n",
        "    window_size = w,\n",
        "    makers = {name:agent for name, agent in agents.items() if name in env.makers},\n",
        "    cci = cci,\n",
        "    makers_belif = {name:agent.weights for name, agent in agents.items() if name in env.makers},\n",
        "    nash_reward = nash_reward,\n",
        "    coll_reward = coll_reward\n",
        ")\n",
        "\n",
        "dir =  saver.save_experiment([env] + list(agents.values()), fig, info)\n",
        "\n",
        "print(json.dumps(info, indent=2))\n",
        "display(fig)\n",
        "print(dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Additional Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "saver = storage.ExperimentStorage('./experiments/exp3/varying_epsilon')\n",
        "\n",
        "objs = saver.load_objects('./experiments/exp3/varying_epsilon/1k')\n",
        "final_cci_1k, final_cum_rewards_1k = objs['final_cci'], scale_rewards_array(objs['final_cum_rewards'],  n_episodes=10)\n",
        "\n",
        "objs = saver.load_objects('./experiments/exp3/varying_epsilon/10k')\n",
        "final_cci_10k, final_cum_rewards_10k = objs['final_cci'], scale_rewards_array(objs['final_cum_rewards'], n_episodes=100)\n",
        "\n",
        "objs = saver.load_objects('./experiments/exp3/varying_epsilon/20k')\n",
        "final_cci_20k, final_cum_rewards_20k = objs['final_cci'], scale_rewards_array(objs['final_cum_rewards'], n_episodes=200)\n",
        "\n",
        "objs = saver.load_objects('./experiments/exp3/varying_epsilon/50k')\n",
        "final_cci_50k, final_cum_rewards_50k = objs['final_cci'], scale_rewards_array(objs['final_cum_rewards'], n_episodes=500)\n",
        "\n",
        "figure, ax1 = plt.subplots()\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "ax1.plot(epsilons, final_cci_1k, color='#1f77b4', label='1k episodes')\n",
        "ax1.plot(epsilons, final_cci_10k, color='#ff7f0e', label='10k episodes')\n",
        "ax1.plot(epsilons, final_cci_20k, color='#18d0f5', label='20k episodes')\n",
        "ax1.plot(epsilons, final_cci_50k, color='#a767e3', label='50k episodes')\n",
        "\n",
        "ax2.scatter(epsilons, final_cum_rewards_1k, color='red', alpha=0.5, s=2, marker='o', edgecolor='none')\n",
        "ax2.scatter(epsilons, final_cum_rewards_10k, color='red', alpha=0.5, s=2, marker='o', edgecolor='none')\n",
        "ax2.scatter(epsilons, final_cum_rewards_20k, color='red', alpha=0.5, s=2, marker='o', edgecolor='none')\n",
        "ax2.scatter(epsilons, final_cum_rewards_50k, color='red', alpha=0.5, s=2, marker='o', edgecolor='none')\n",
        "\n",
        "ax1.axhline(0, linestyle='--', color='black', label='Nash')\n",
        "ax1.axhline(1, linestyle='--', color='green', label='Coll')\n",
        "\n",
        "ax1.axvline(MakerEXP3.compute_epsilon(3,  1_000*0.25), linestyle=':', color='#1f77b4')\n",
        "ax1.axvline(MakerEXP3.compute_epsilon(3, 10_000*0.25), linestyle=':', color='#ff7f0e')\n",
        "ax1.axvline(MakerEXP3.compute_epsilon(3, 20_000*0.25), linestyle=':', color='#18d0f5')\n",
        "ax1.axvline(MakerEXP3.compute_epsilon(3, 50_000*0.25), linestyle=':', color='#a767e3')\n",
        "\n",
        "ax1.set_xlabel('Epsilons')\n",
        "ax1.set_ylabel('CCI')\n",
        "ax2.set_ylabel('Scaled Cumulative Reward')\n",
        "ax1.set_title('(Min) CCI and Cumulative Rewards wrt Epsilon')\n",
        "\n",
        "lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "ax1.legend(lines1 + lines2, labels1 + labels2, loc='best', ncol=3)\n",
        "ax1.grid(True)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "saver.save_figures({'figure': figure})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOmfK9UlpCsbmt8VYJqHDlz",
      "collapsed_sections": [
        "DResD4xyGhBX",
        "udKsn5GQHHt_"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rlvenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
