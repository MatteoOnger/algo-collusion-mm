{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatteoOnger/algo-collusion-mm/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMTOP3utEbcK"
      },
      "source": [
        "# Algorithmic Collusion in Market Making - EXP3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGZMs7D1J3fF"
      },
      "source": [
        "A notebook testing various EXP3 agents implementing market-making strategies in the Glosten-Milgrom environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxmHCO7rFONX"
      },
      "source": [
        "## Notebook Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DResD4xyGhBX"
      },
      "source": [
        "### Colab Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOP-nvvbEYvD"
      },
      "outputs": [],
      "source": [
        "# Do NOT run this cell in local environment - it's intended for Google Colab only.\n",
        "\n",
        "# Clone GitHub repository\n",
        "!git clone https://github.com/MatteoOnger/algo-collusion-mm.git\n",
        "\n",
        "# Install dependencies\n",
        "!pip install --quiet -r /content/algo-collusion-mm/requirements.txt\n",
        "\n",
        "# Set working directory\n",
        "%cd /content/algo-collusion-mm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udKsn5GQHHt_"
      },
      "source": [
        "### Local Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XOKNp7tDFKHo"
      },
      "outputs": [],
      "source": [
        "# Do NOT run this cell in Google Colab - it's intended for local Jupyter Notebooks only.\n",
        "\n",
        "# Autoreload imports\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# Select interactive backend for matplotlib\n",
        "%matplotlib widget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1Ulv9GBHZaH"
      },
      "source": [
        "## Main Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxDHqYnXrDzE"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import src.utils.gtu as gtu\n",
        "import src.utils.plots as plots\n",
        "import src.utils.storage as storage\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from src.agents.agent import Agent\n",
        "from src.agents.makers.exp3 import MakerEXP3\n",
        "from src.agents.traders.nopass import NoPassTrader\n",
        "from src.envs import GMEnv\n",
        "from src.utils.common import scale_rewards_array, get_calvano_collusion_index\n",
        "from src.utils.stats import OnlineVectorStats\n",
        "\n",
        "\n",
        "plots.DECIMAL_PLACES_VALUES = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "saver = storage.ExperimentStorage('./experiments/exp3')\n",
        "\n",
        "objects = saver.load_objects('')\n",
        "print(f'Objects loaded: {list(objects.keys())}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multiple Runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80Oq4lNWrKcl"
      },
      "outputs": [],
      "source": [
        "r = 100             # Number of experiments\n",
        "n = 25_000          # Number of episodes\n",
        "k = 100             # Number of windows\n",
        "w = n // k          # Window size\n",
        "\n",
        "n_makers = 2        # Number of market makers\n",
        "\n",
        "nash_reward = 0.1   # Nash reward (single-agent case)\n",
        "coll_reward = 0.5   # Collusive reward (single-agent case)\n",
        "\n",
        "# Prices and action space of the market makers\n",
        "# action_space = np.array([[0.0, 0.0], [.6, .4], [.68, .32]])\n",
        "prices =  np.round(np.arange(0.0, 1.0 + 0.2, 0.2), 2)\n",
        "action_space = np.array([(ask, bid) for ask in prices for bid in prices if (ask  > bid)])\n",
        "\n",
        "# Min CCI of the last window per each experiment\n",
        "final_cci = np.zeros(r)\n",
        "# Min cumulative reward of the last window per each experiment\n",
        "final_cum_rewards = np.zeros(r)\n",
        "\n",
        "# To compute online statistics\n",
        "stats_cci = OnlineVectorStats(n_makers)\n",
        "stats_rwd = OnlineVectorStats(n_makers)\n",
        "\n",
        "# To save experimental results\n",
        "saver = storage.ExperimentStorage('./experiments/exp3')\n",
        "\n",
        "start_time = time.time()\n",
        "current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "saver.print_and_save(f'Started at {current_time}')\n",
        "\n",
        "for i in range(r):\n",
        "    if i % 10 == 0:\n",
        "        saver.print_and_save(f'Running {i} ...')\n",
        "\n",
        "    agents: dict[str, Agent] = {\n",
        "        'maker_u_0': MakerEXP3(epsilon=MakerEXP3.compute_epsilon(len(action_space), n), scale_rewards=lambda r: (r / 0.3), action_space=action_space, name='maker_u_0'),\n",
        "        'maker_u_1': MakerEXP3(epsilon=MakerEXP3.compute_epsilon(len(action_space), n), scale_rewards=lambda r: (r / 0.3), action_space=action_space, name='maker_u_1'),\n",
        "        'trader_0': NoPassTrader(name='trader_0', tie_breaker='rand'),\n",
        "    }\n",
        "\n",
        "    env = GMEnv(\n",
        "        generate_vt = lambda: 0.5,\n",
        "        n_episodes = n,\n",
        "        n_makers_u = n_makers,\n",
        "        n_makers_i = 0,\n",
        "        n_traders = 1,\n",
        "    )\n",
        "\n",
        "    _, info = env.reset()\n",
        "\n",
        "    for agent in env.agent_iter():\n",
        "        action = agents[agent].act(env.observe(agent))\n",
        "        _, rewards, _, _, infos = env.step(action)\n",
        "\n",
        "        if infos['episode_finished']:\n",
        "            for a in env.possible_agents:\n",
        "                agents[a].update(rewards[a], infos[a])\n",
        "\n",
        "    # Compute calvano collusion idex per window and agent\n",
        "    cci = get_calvano_collusion_index(\n",
        "        np.array([agents[name].history.get_rewards() for name in env.makers]),\n",
        "        nash_reward = nash_reward,\n",
        "        coll_reward = coll_reward,\n",
        "        window_size = w\n",
        "    )\n",
        "\n",
        "    # Collect info\n",
        "    info = {\n",
        "        'parmas' : {\n",
        "            'n_episodes' : n,\n",
        "            'window_size' : w,\n",
        "            'action_space' : str(action_space).replace('\\n', ','),\n",
        "            'epsilon' : [agents[name].epsilon for name in env.makers],\n",
        "            'agent_type' : [agent.__class__.__name__ for agent in agents.values()]\n",
        "        },\n",
        "        'freq_actions' : {\n",
        "            0 : {name : str(agents[name].history.compute_freqs(slice(0, w))).replace('\\n', '') for name in env.makers},\n",
        "            k : {name : str(agents[name].history.compute_freqs(slice(-w, None))).replace('\\n', '') for name in env.makers},\n",
        "            'global' : {name : str(agents[name].history.compute_freqs()).replace('\\n', '') for name in env.makers}\n",
        "        },\n",
        "        'most_common_action' : {\n",
        "            0 : {name : str(agents[name].history.compute_most_common(slice(0, w))) for name in env.makers},\n",
        "            k : {name : str(agents[name].history.compute_most_common(slice(-w, None))) for name in env.makers},\n",
        "            'global' : {name : str(agents[name].history.compute_most_common()) for name in env.makers}\n",
        "        },\n",
        "        'cumulative_rewards' : {\n",
        "            0 : {name : round(float(agent.history.get_rewards(slice(0, w)).sum()), 3) for name, agent in agents.items()},\n",
        "            k : {name : round(float(agent.history.get_rewards(slice(-w, None)).sum()), 3) for name, agent in agents.items()},\n",
        "            'global' : env.cumulative_rewards\n",
        "        },\n",
        "        'cci' : {\n",
        "            0  : {name : round(float(cci[idx, 0]), 3) for idx, name in enumerate(env.makers)},\n",
        "            k  : {name : round(float(cci[idx, -1]), 3) for idx, name in enumerate(env.makers)},\n",
        "            'global' : {name : round(float(cci[idx, :].mean()), 3) for idx, name in enumerate(env.makers)},\n",
        "        },\n",
        "        'seed' : {\n",
        "            name : agent._seed for name, agent in agents.items()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Update statistics\n",
        "    stats_cci.update(cci[:, -1])\n",
        "    stats_rwd.update(np.array([env.cumulative_rewards[name] for name in env.makers]))\n",
        "\n",
        "    # Update last window's statistics\n",
        "    final_cci[i] = cci[:, -1].min()\n",
        "    final_cum_rewards[i] = np.array([agents[name].history.get_rewards(slice(-w, None)).sum() for name in env.makers]).min()\n",
        "\n",
        "    # Save and print results\n",
        "    dir = saver.save_experiment([env] + list(agents.values()), info=info)\n",
        "    saver.print_and_save(f'{(i+1):03} {\"*\" if cci[0, -1] >= 0.45 or cci[1, -1] >= 0.45 else \" \"} -> CCI:{info[\"cci\"][n//w]}'.ljust(60) + f' ({dir})')\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "saver.print_and_save(f'Done at {current_time} | Execution time: {execution_time:.2f} seconds')\n",
        "\n",
        "# Save and print results\n",
        "saver.save_objects({'final_cci': final_cci, 'final_cum_rewards': final_cum_rewards})\n",
        "saver.print_and_save(\n",
        "    f'Results:\\n'\n",
        "    f'- [CCI] Average in the last window: {np.round(stats_cci.get_mean(), 4)}\\n'\n",
        "    f'- [CCI] Standard deviation in the last window: {np.round(stats_cci.get_std(), 4)}\\n'\n",
        "    f'- [RWD] Global average: {np.round(stats_rwd.get_mean(), 4)}\\n'\n",
        "    f'- [RWD] Global standard deviation: {np.round(stats_rwd.get_std(), 4)}'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Single Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "saver = storage.ExperimentStorage('./experiments/exp3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n = 25_000          # Number of episodes\n",
        "k = 100             # Number of windows\n",
        "w = n // k          # Window size\n",
        "\n",
        "n_makers = 2        # Number of market makers\n",
        "\n",
        "nash_reward = 0.1   # Nash reward (single-agent case)\n",
        "coll_reward = 0.5   # Collusive reward (single-agent case)\n",
        "\n",
        "counter = 0         # Number of episodes done\n",
        "\n",
        "# Prices and action space of the market makers\n",
        "# action_space = np.array([[0.0, 0.0], [.6, .4], [.68, .32]])\n",
        "prices =  np.round(np.arange(0.0, 1.0 + 0.2, 0.2), 2)\n",
        "action_space = np.array([(ask, bid) for ask in prices for bid in prices if (ask  > bid)])\n",
        "\n",
        "agents: dict[str, Agent] = {\n",
        "    'maker_u_0': MakerEXP3(epsilon=MakerEXP3.compute_epsilon(len(action_space), n), scale_rewards=lambda r: (r / 0.3), action_space=action_space, name='maker_u_0'),\n",
        "    'maker_u_1': MakerEXP3(epsilon=MakerEXP3.compute_epsilon(len(action_space), n), scale_rewards=lambda r: (r / 0.3), action_space=action_space, name='maker_u_1'),\n",
        "    'trader_0': NoPassTrader(tie_breaker='rand', name='trader_0'),\n",
        "}\n",
        "\n",
        "env = GMEnv(\n",
        "    generate_vt = lambda: 0.5,\n",
        "    n_episodes = n,\n",
        "    n_makers_u = n_makers,\n",
        "    n_makers_i = 0,\n",
        "    n_traders = 1,\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "print(f'Started at {current_time}')\n",
        "\n",
        "_, info = env.reset()\n",
        "for agent in env.agent_iter():\n",
        "    action = agents[agent].act(env.observe(agent))\n",
        "    _, rewards, _, _, infos = env.step(action)\n",
        "\n",
        "    if infos['episode_finished']:\n",
        "        if counter % 10_000 == 0:\n",
        "            print(f'Running episode {counter} ...')\n",
        "\n",
        "        for a in env.possible_agents:\n",
        "            # Save the current belif of the agent\n",
        "            if a in env.makers and counter % (k//5) == 0:\n",
        "                agents[a].history.record_extra(agents[a].weights.copy())\n",
        "            \n",
        "            agents[a].update(rewards[a], infos[a])\n",
        "    \n",
        "        counter += 1\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "print(f'Done at {current_time} | Execution time: {execution_time:.2f} seconds')\n",
        "\n",
        "# Compute calvano collusion idex per window and agent\n",
        "cci = get_calvano_collusion_index(\n",
        "    np.array([agent.history.get_rewards() for name, agent in agents.items() if name in env.makers]),\n",
        "    nash_reward = nash_reward,\n",
        "    coll_reward = coll_reward,\n",
        "    window_size = w\n",
        ")\n",
        "\n",
        "# Collect info\n",
        "info = {\n",
        "    'parmas' : {\n",
        "        'n_episodes' : n,\n",
        "        'window_size' : w,\n",
        "        'action_space' : str(action_space).replace('\\n', ','),\n",
        "        'epsilon' : [agents[name].epsilon for name in env.makers],\n",
        "        'tie_breaker' : [agents[name].tie_breaker for name in env.traders],\n",
        "        'agent_type' : [agent.__class__.__name__ for agent in agents.values()]\n",
        "    },\n",
        "    'freq_actions' : {\n",
        "        0 : {name : str(agents[name].history.compute_freqs(slice(0, w))).replace('\\n', '') for name in env.makers},\n",
        "        k : {name : str(agents[name].history.compute_freqs(slice(-w, None))).replace('\\n', '') for name in env.makers},\n",
        "        'global' : {name : str(agents[name].history.compute_freqs()).replace('\\n', '') for name in env.makers}\n",
        "    },\n",
        "    'most_common_action' : {\n",
        "        0 : {name : str(agents[name].history.compute_most_common(slice(0, w))) for name in env.makers},\n",
        "        k : {name : str(agents[name].history.compute_most_common(slice(-w, None))) for name in env.makers},\n",
        "        'global' : {name : str(agents[name].history.compute_most_common()) for name in env.makers}\n",
        "    },\n",
        "    'cumulative_rewards' : {\n",
        "        0 : {name : round(float(agent.history.get_rewards(slice(0, w)).sum()), 3) for name, agent in agents.items()},\n",
        "        k : {name : round(float(agent.history.get_rewards(slice(-w, None)).sum()), 3) for name, agent in agents.items()},\n",
        "        'global' : env.cumulative_rewards\n",
        "    },\n",
        "    'cci' : {\n",
        "        0  : {name : round(float(cci[idx, 0]), 3) for idx, name in enumerate(env.makers)},\n",
        "        k  : {name : round(float(cci[idx, -1]), 3) for idx, name in enumerate(env.makers)},\n",
        "        'global' : {name : round(float(cci[idx, :].mean()), 3) for idx, name in enumerate(env.makers)},\n",
        "    },\n",
        "    'seed' : {\n",
        "        name : agent._seed for name, agent in agents.items()\n",
        "    }\n",
        "}\n",
        "\n",
        "# Plot figure\n",
        "fig = plots.plot_all(\n",
        "    window_size = w,\n",
        "    makers = {name:agent for name, agent in agents.items() if name in env.makers},\n",
        "    cci = cci,\n",
        "    makers_belif = {name:agent.weights for name, agent in agents.items() if name in env.makers},\n",
        "    nash_reward = nash_reward,\n",
        "    coll_reward = coll_reward,\n",
        "    title = 'EXP3 Makers Summary Plots'\n",
        ")\n",
        "\n",
        "# Save results\n",
        "dir =  saver.save_experiment([env] + list(agents.values()), fig, info)\n",
        "\n",
        "print(json.dumps(info, indent=2))\n",
        "display(fig)\n",
        "print(dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Additional Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epsilons = np.arange(1, 100) / 100\n",
        "\n",
        "saver = storage.ExperimentStorage('./experiments/exp3/varying_epsilon')\n",
        "\n",
        "objs = saver.load_objects('./experiments/exp3/varying_epsilon')\n",
        "final_cci, final_cum_rewards = objs['final_cci'], scale_rewards_array(objs['final_cum_rewards'],  n_episodes=20_000)\n",
        "\n",
        "plt.close()\n",
        "\n",
        "figure, ax1 = plt.subplots()\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "ax1.plot(epsilons, final_cci, color='#1f77b4', label='1k episodes')\n",
        "\n",
        "ax2.scatter(epsilons, final_cum_rewards, color='red', alpha=0.5, s=2, marker='o', edgecolor='none')\n",
        "\n",
        "ax1.axhline(0, linestyle='--', color='black', label='Nash')\n",
        "ax1.axhline(1, linestyle='--', color='green', label='Coll')\n",
        "\n",
        "ax1.axvline(MakerEXP3.compute_epsilon(3,  20_000*0.25), linestyle=':', color='#1f77b4')\n",
        "\n",
        "ax1.set_xlabel('Epsilon')\n",
        "ax1.set_ylabel('CCI')\n",
        "ax2.set_ylabel('Scaled Cumulative Reward')\n",
        "ax1.set_title('(Min) CCI and Cumulative Rewards wrt Epsilon')\n",
        "\n",
        "lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "ax1.legend(lines1 + lines2, labels1 + labels2, loc='best', ncol=3)\n",
        "ax1.grid(True)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "saver.save_figures({'figure': figure})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent_name = 'maker_u_0'\n",
        "agent = agents[agent_name]\n",
        "\n",
        "plt.close()\n",
        "\n",
        "plots.plot_maker_belif_evolution(\n",
        "    indexes = agent.price_to_index(agent.action_space),\n",
        "    labels = agent.prices,\n",
        "    values = np.stack(agent.history.get_extras()),\n",
        "    log_scale = False,\n",
        "    adaptive_scale = False,\n",
        "    agent_name = agent_name,\n",
        ")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.close()\n",
        "\n",
        "plots.plot_makers_best_actions(\n",
        "    true_value = 0.5,\n",
        "    actions = np.stack([objects['maker_u_0'].history.get_actions(), objects['maker_u_1'].history.get_actions()]),\n",
        "    agents_name = list(objects.keys())\n",
        ")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Offline Game Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find all pure Nash equilibria (NE) and pure coarse correlated equilibria (CCE)\n",
        "# prices =  np.round(np.arange(0.0, 1.0 + 0.2, 0.2), 2)\n",
        "# action_space = np.array([(ask, bid) for ask in prices for bid in prices if (ask  > bid)])\n",
        "# action_space = np.array([[.6, .4], [.8, .2]])\n",
        "\n",
        "n_makers = 2\n",
        "action_spaces = np.repeat(action_space[None, :], repeats=n_makers, axis=0)\n",
        "\n",
        "print(f'Action spaces shape: {action_spaces.shape}')\n",
        "\n",
        "joint_action_space, rewards = gtu.compute_joint_actions_and_rewards(action_spaces, true_value=0.5, tie_breaker='rand')\n",
        "\n",
        "print(f'Joint action space shape: {joint_action_space.shape}')\n",
        "print(f'Rewards shape: {rewards.shape}')\n",
        "print('--------------------------------------------')\n",
        "\n",
        "scaled_rewards = rewards\n",
        "\n",
        "start_time = time.time()\n",
        "print('Search pure CCEs:')\n",
        "\n",
        "for a in itertools.product(*[range(s) for s in rewards.shape[:-1]]):\n",
        "    prof = np.zeros(rewards.shape[:-1])\n",
        "    prof[a] = 1.0\n",
        "    if gtu.is_cce(scaled_rewards, prof):\n",
        "        print(f\"- {a} -> {str(joint_action_space[a].swapaxes(-1, -2)).replace('\\n', '')} is a CCE\")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "print(f'Done at {current_time} | Execution time: {execution_time:.2f} seconds')\n",
        "print('--------------------------------------------')\n",
        "\n",
        "start_time = time.time()\n",
        "print('Search pure NEs:')\n",
        "\n",
        "for a in itertools.product(*[range(s) for s in rewards.shape[:-1]]):\n",
        "    prof = np.zeros((n_makers, len(action_space)))\n",
        "    prof[np.arange(n_makers), a] = 1.0\n",
        "    if gtu.is_ne(scaled_rewards, prof):\n",
        "        print(f\"- {a} -> {str(joint_action_space[a].swapaxes(-1, -2)).replace('\\n', '')} is a NE\")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "print(f'Done at {current_time} | Execution time: {execution_time:.2f} seconds')\n",
        "print('--------------------------------------------')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOmfK9UlpCsbmt8VYJqHDlz",
      "collapsed_sections": [
        "DResD4xyGhBX",
        "udKsn5GQHHt_"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rlvenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
